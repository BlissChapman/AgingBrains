{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torchvision.utils import save_image\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from models.GreyUTKFace import VAE\n",
    "from models.GreyUTKFace import AgeClassifier\n",
    "import os\n",
    "from processed_data import GreyUTKFace\n",
    "from utils import device\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GreyUTKFace.Dataset(train=False, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.min_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.max_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE.Model()\n",
    "model.to(device)\n",
    "model.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Latent Point Average For Each Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_ages = []\n",
    "\n",
    "latent_ages.append(np.zeros(model.latent_space))\n",
    "\n",
    "for age in range(test_dataset.min_age, test_dataset.max_age + 1):\n",
    "    items = test_dataset.get_faces_of_age(age)\n",
    "    faces = list(map(lambda item: item[0].numpy(), items))\n",
    "    if len(faces) == 0:\n",
    "        latent_ages.append(np.zeros(model.latent_space))\n",
    "        continue\n",
    "    faces = np.array(faces)\n",
    "    face_imgs = torch.from_numpy(faces).to(device)\n",
    "    means, _ = model.encode(face_imgs)\n",
    "    latent_ages.append(np.average(means.detach().cpu().numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Many Steps between Ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_age = 10\n",
    "smooth_latent_ages = []\n",
    "for i in range(len(latent_ages) - 1):\n",
    "    age = latent_ages[i]\n",
    "    next_age = latent_ages[i+1]\n",
    "    \n",
    "    smooth_latent_ages.append(age)\n",
    "    \n",
    "    if steps_per_age <= 1:\n",
    "        continue\n",
    "        \n",
    "    step_vector = next_age - age / steps_per_age\n",
    "    curr = age\n",
    "    for i in range(steps_per_age - 1):\n",
    "        curr += step_vector\n",
    "        smooth_latent_ages.append(curr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.6775, -0.0817, -0.7176,  ..., -0.1975, -0.6193, -0.0118],\n",
       "        [ 0.7834, -0.1547, -0.6889,  ..., -0.0632, -0.4801,  0.0488],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.3699, -0.1727, -0.7827,  ..., -0.3762, -0.1263, -1.0620]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_latent_ages = torch.from_numpy(np.array(smooth_latent_ages)).to(device).float()\n",
    "tensor_latent_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_ages = model.decode(tensor_latent_ages).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model.dir + 'weights/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(model.dir + 'results/gif/')\n",
    "except:\n",
    "    pass\n",
    "for i in range(len(tensor_latent_ages)):\n",
    "    plt.figure()\n",
    "    plt.imshow(face_ages[i].view(model.input_w, model.input_h).detach().numpy())\n",
    "    plt.title('Age: {}'.format(round(i / steps_per_age, 2)))\n",
    "    plt.savefig(model.dir + 'results/gif/{}.png'.format(i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert @models/GreyUTKFace/VAE/results/gif/image_list.txt models/GreyUTKFace/VAE/results/age.gif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "gif_path = model.dir + 'results/age.gif'\n",
    "img_list_file_path = model.dir + 'results/gif/image_list.txt'\n",
    "file_list = glob.glob(model.dir + 'results/gif/*.png') # Get all the pngs in the current directory\n",
    "list.sort(file_list, key=lambda x: int(x.split('.png')[0].split('/')[-1])) # Sort the images by #, this may need to be tweaked for your use case\n",
    "\n",
    "with open(img_list_file_path, 'w') as file:\n",
    "    for item in file_list:\n",
    "        file.write(\"%s\\n\" % item)\n",
    "command = 'convert @{} {}'.format(img_list_file_path, gif_path)\n",
    "print(command)\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Classifier Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_classifier = AgeClassifier.Model()\n",
    "age_classifier.to(device)\n",
    "age_classifier.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (fc1): Linear(in_features=12544, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
